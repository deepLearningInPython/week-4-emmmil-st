{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":763778,"sourceType":"datasetVersion","datasetId":362178},{"sourceId":112347694,"sourceType":"kernelVersion"}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n**This exercise is part of the [Recurrent network for entailment](https://www.kaggle.com/code/datasniffer/nlp-recurrent-networks-for-entailment/notebook).**\n\n---\n\n# How advanced language models generate text\n\n<!--div style=\"display:block;width:300px;float:right;height:40%\">&nbsp;</div-->\n\nThe BERT model [is _non-causal_](https://huggingface.co/blog/bert-101), meaning that it has access to future words (or _tokens_, rather) to predict the likelihood of other words in context. A purely causal model is GPT2. This means that GPT2 only has has access to past words in a sequence to predict future words (hence the term \"causal\"). We will use it to demonstrate how neural networks can generate realistic sounding text. \n\n\nLet's get started. First, run the next code cell.\n    \n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools_nlp_utility import *\n\nprint(\"\\nSetup complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:46:39.447478Z","iopub.execute_input":"2024-11-22T21:46:39.448266Z","iopub.status.idle":"2024-11-22T21:46:57.883607Z","shell.execute_reply.started":"2024-11-22T21:46:39.448170Z","shell.execute_reply":"2024-11-22T21:46:57.882635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# GPT2\n\nGPT stands for Generative Pre-trained Transformer. _Generative_ implies that the model intends to be represent the full joint probability density of every possible sequence of words (or _tokens_, rather), instead of only modeling the conditional probability density of a single word given a context.\n\nRecurrent neural network layers for contextualizing word embeddings are slow. This is mainly because each recurrent layer needs to iteratively process one word embedding after the other, in sequence. This fundamentally prevents paralellization of the computations on multiple processor cores or on GPUs and/or TPUs. Instead of RNN layers, GPT2 uses a mechanism called \"_self attention_\" which does allow for parallelization. \"Self attention\" is similar but not identical to the gating process that takes place in LSTMs and GRUs: The sequence of input embeddings is turned into a new sequence of embeddings in the form of weighted averages of the input embeddings. The idea is that each weighted average will represent a different semantic unit in the input sequence. The weights for computing these averages are computed from the input embeddings using a feed forward layer that is simultaneously trained with the entire network. The output from this is called an \"_attention head_\", and often multiple such attention heads are run in parallel, in which case the outputs are concatenated. If the (concatenated) sequence of attention head outputs is then passed through a feed forward layer, we speak of a (self-attending) \"_transformer_\" block. GPT2 (as well as BERT) stacks multiple such transformer blocks on top of each other; hence the term Transformer in its name. \n\n<details><summary>Self-attention and transformer in some detail</summary>\nLet $u_j, j=0, \\ldots, n-1$ is the sequence of word embeddings, and let $U = [u_0, u_2, \\ldots, u_{n-1}]$. Then the attention weights are computed from\n<br /><br />\n<!--\n$$W_\\text{attention} = \\mathrm{softmax}((W_\\text{q}U+b_\\text{q})'(W_\\text{k}U + b_\\text{k})\n= \\mathrm{softmax}((U'W_\\text{q}'+1 b_\\text{q}')(W_\\text{k}U + b_\\text{k}1')) \\\\\n= \\mathrm{softmax}(U'W_\\text{q}'(W_\\text{k}U + b_\\text{k}1')+1 b_\\text{q}'(W_\\text{k}U + b_\\text{k}1')) \\\\\n= \\mathrm{softmax}(U'W_\\text{q}'W_\\text{k}U + U'W_\\text{q}'b_\\text{k}1' + 1 b_\\text{q}'W_\\text{k}U + 1 b_\\text{q}'b_\\text{k}1') \\\\\n$$\n-->\n\n$$\n\\begin{align*}\nQ &= W_\\text{q} U + b_\\text{q} 1_n' \\\\\nK &= W_\\text{k} U + b_\\text{k} 1_n' \\\\\nV &= W_\\text{v} U + b_\\text{v} 1_n' \\\\\nW_\\text{attention} &= \\mathrm{softmax}(Q'K / \\sqrt{d}) \n\\end{align*}\n$$\n    \nand the output sequence from the attention head is computed as\n    \n$$O = (W_\\text{v}U + b_\\text{v}1') W_\\text{attention}$$\n    \nHere $d$ is the number of rows of $Q$ and $K$ (which may or may not be the same as the number of rows of $U$â€”i.e. the input word embedding dimension). Furthermore, the $\\mathrm{softmax}$ function is normalizing _by row_ in the matrix $Q'V/\\sqrt{d}$, and _not_ over all the values in the matrix. If there are multiple _attention heads_ with outputs $O_1, O_2, \\ldots, O_h$, each with their own sets of parameters $W_\\text{q}, b_\\text{q}, W_\\text{k}, b_\\text{k}, W_\\text{v}, b_\\text{v}$, then these outputs are concatenated:\n\n$$ O = \\begin{pmatrix} O_1 \\\\ O_2 \\\\ \\vdots \\\\ O_h \\end{pmatrix}. $$\n\nWhen $O$ is passed through a dense feed forward layer and normalized, this constitutes a _transformer_ block:\n\n$$ T = W_\\text{transformer}\\sigma(W_\\text{ffw} O + b_\\text{ffw}) + b_\\text{transformer}, $$\n\nwhere $\\sigma$ can be any activation function, but usually is a ReLU or GeLU activation function.\n</details><br />\n\n    \nWe can import a pretrained version of a GPT2 model from the HuggingFace `transformer` library:\n    \n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModelForCausalLM\ngpt2_model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\ngpt2_model.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:46:57.885395Z","iopub.execute_input":"2024-11-22T21:46:57.886111Z","iopub.status.idle":"2024-11-22T21:47:23.158975Z","shell.execute_reply.started":"2024-11-22T21:46:57.886071Z","shell.execute_reply":"2024-11-22T21:47:23.158210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 1: Tokenization and encoding\n\nAs is always the case with text, we need to turn text into a sequence of tokens, and then encode those sequences into a TensorFlow friendly format: `numpy` arrays (or `tensorflow` tensors). The easiest is to use the tokenizers that come with pretrained models from the `transformer` library:\n    \n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:23.160064Z","iopub.execute_input":"2024-11-22T21:47:23.160346Z","iopub.status.idle":"2024-11-22T21:47:25.153761Z","shell.execute_reply.started":"2024-11-22T21:47:23.160313Z","shell.execute_reply":"2024-11-22T21:47:25.152715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq = \"Machine learning with TensorFlow can do amazing\" # unfinished sentence\ninpts = tokenizer(seq, return_tensors=\"tf\")\ninpts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:25.155595Z","iopub.execute_input":"2024-11-22T21:47:25.155915Z","iopub.status.idle":"2024-11-22T21:47:25.174330Z","shell.execute_reply.started":"2024-11-22T21:47:25.155884Z","shell.execute_reply":"2024-11-22T21:47:25.173612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe tokenizer knows words, as well as parts of words, so that also unfamiliar words can be represented. In fact, it even encodes single characters and may consider a space to be part of a token:\n    \n","metadata":{}},{"cell_type":"code","source":"inpt_ids = inpts[\"input_ids\"]  # just token IDs, no attention mask\n\nprint(f\"{'token id': >10} {'token ': >13}\\n{'  ':-<10} {'  ':-<13}\")\nfor id in inpt_ids[0].numpy():\n    word = tokenizer.decode(id)\n    print(f\"{id: >10} {'`'+word+'`': >13}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:25.175284Z","iopub.execute_input":"2024-11-22T21:47:25.175526Z","iopub.status.idle":"2024-11-22T21:47:26.697258Z","shell.execute_reply.started":"2024-11-22T21:47:25.175504Z","shell.execute_reply":"2024-11-22T21:47:26.696393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe tokenizer has attributes regarding the vocabulary, such as the `vocab` attribute. Can you figure out how many tokens does the vocabulary of the tokenizer know?\n    \n","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\npart_1.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:26.698502Z","iopub.execute_input":"2024-11-22T21:47:26.699567Z","iopub.status.idle":"2024-11-22T21:47:26.707782Z","shell.execute_reply.started":"2024-11-22T21:47:26.699525Z","shell.execute_reply":"2024-11-22T21:47:26.706815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Predicting the _next word_ probabilities\n\n\nWe can pass the tokenized and encoded string directly into `gpt2_model`. The output is an object that has an `logits` attribute. Logits are the network outputs _before_ applying the _softmax_ activation function. The `logits` attribute is an array with `shape = [batch_size, seq_len, vocab_size]`, and so for a single input string it is essentially a matrix of dimensions `seq_len` by `vocab_size`.\n\n","metadata":{}},{"cell_type":"code","source":"output = gpt2_model(inpt_ids)\noutput.logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:26.708891Z","iopub.execute_input":"2024-11-22T21:47:26.709157Z","iopub.status.idle":"2024-11-22T21:47:28.758006Z","shell.execute_reply.started":"2024-11-22T21:47:26.709134Z","shell.execute_reply":"2024-11-22T21:47:28.757026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe output format is described summarily in  the [documentation](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions).\n\nWhat is important to understand is that each row in the matrix specifies the _conditional_ probability for the next token _given_ the sequence of tokens so far, for each token in the vocabulary. For example, the second row encodes the probability $P(T\\, |\\, \\mathtt{Machine, learning})$ where $T$ is any token in the vocabulary, and hence, it can be used to compute the probability $P(\\mathtt{will}\\,|\\, \\mathtt{Machine, learning})$ for instance, or the probability $P(\\mathtt{has}\\, |\\, \\mathtt{Machine, learning})$. The token $T$ for which this _conditional_ probability is the highest, is the most likely token to follow the sequence `[Machine, learning]`.\n\nThe last row in the matrix specifies the _conditional_ probability for the next token _given_ the entire sequence so far: \n\n$$P(T\\,|\\,\\mathtt{Machine, learning, with, T, ensor, Flow, can, do, amazing}),$$\n\nwhere $T$ is a token in the vocabulary. (If you wonder about the split between `T`, `ensor`, and `Flow`, look back at the encoding above.)\n\nSo why does `logits` not just give the last row of this matrix, but all the intermediate probabilities $P(T|\\mathtt{Machine}), P(T|\\mathtt{Machine, learning}), P(T|\\mathtt{Machine, learning, with})$, etc. as well? \n\nThe answer lies in the fact that GPT2 is a _generative language model_, meaning that it wants to give the _joint_ probability $P(\\mathtt{Machine, learning, with, T, ensor, Flow, can, do, amazing})$ and not (just) the conditional probability from the last row: The matrix of logits encodes the entire probability of observing the sequence of tokens. Recall from probability theory that \n\n$$\n\\begin{align}\n    P(&\\mathtt{Machine,learning,with,\\ldots,amazing}) \\\\\n    &= P(\\mathtt{Machine})\\cdot P(\\mathtt{learning,with,\\ldots,do,amazing}|\\mathtt{Machine})\\, \\\\\n    &= P(\\mathtt{Machine})\\cdot P(\\mathtt{learning}|\\mathtt{Machine})\\cdot P(\\mathtt{with,\\ldots,do,amazing}|\\mathtt{Machine, learning}) \\\\\n    &= P(\\mathtt{Machine})\\cdot P(\\mathtt{learning}|\\mathtt{Machine})\\cdot P(\\mathtt{with}|\\mathtt{Machine,learning})\n    \\cdots P(\\mathtt{amazing}|\\mathtt{Machine,learning,with,\\ldots,do})\n\\end{align}\n$$\n\nNotice that all of the conditional probabilities on the right hand side of the last line are represented by the rows of the `logits` attribute.\n\n#### The token predicted to follow 'amazing'\n\n\n    \nLet's try to find the token $T$ in the vocabulary that is the most likely to follow the text \"_Machine learning with TensorFlow can do amazing_\" according to GPT2. To a human this sentence screams that the word 'things' should follow, but does GPT2 also think so too?\n\n\n    \nTo do so, we can take the last row from `output.logits`, compute the conditional probabilties and find the token for which this is largest. To compute the probabilities we can pass the logits through the `keras.activations.softmax()` function. Then TensorFlow's `tf.argmax()` function returns the index where this probabilitie is highest:\n    \n","metadata":{}},{"cell_type":"code","source":"last_logits = output.logits[0,-1:,:]\nconditional_probs = tf.keras.activations.softmax(last_logits)\ntoken_id = tf.argmax(conditional_probs, axis=1)\ntokenizer.decode(token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:28.759059Z","iopub.execute_input":"2024-11-22T21:47:28.759315Z","iopub.status.idle":"2024-11-22T21:47:28.779779Z","shell.execute_reply.started":"2024-11-22T21:47:28.759292Z","shell.execute_reply":"2024-11-22T21:47:28.778922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nSo indeed GPT2 also thinks 'things' should follow the sentence! (The example was found in [this post](https://jamesmccaffrey.wordpress.com/2021/10/21/a-predict-next-word-example-using-hugging-face-and-gpt-2/).)\n\n    \nThe above code does one thing that isn't necessary: passing the logits through the _softmax_ function. In stead of searching for the token with the highest conditional probability, we can also simply search for the token with the highest logit.\n\nLet's do this for all the rows in `output.logits` to see the predicted token from the sequence of conditional probabilities that it represents:\n    \n","metadata":{}},{"cell_type":"code","source":"# find the most likely next token IDs for each row of logits\npred_ids = tf.argmax(output.logits[0,:,:],axis=1)\n\n# print the most likely token together with the sequence of tokens so far\nfor i, pred_id in enumerate(pred_ids):\n    \n    # decode the sequence of tokens so far\n    past = \"'\" + tokenizer.decode(inpt_ids.numpy()[0][:i+1]) + \"'\"\n    \n    # decode the predicted next token\n    pred_token = \"'\" + tokenizer.decode(pred_id) + \"'\"\n    \n    print(f\"{i}: {past: <50} â†’ {pred_token: <12}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:47:28.780779Z","iopub.execute_input":"2024-11-22T21:47:28.781056Z","iopub.status.idle":"2024-11-22T21:47:28.797208Z","shell.execute_reply.started":"2024-11-22T21:47:28.781033Z","shell.execute_reply":"2024-11-22T21:47:28.796505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nNote that the predicted output is always a plausible next word or token, except perhaps for the token predicted after 'Machine' (a period) and after 'Machine learning with TensorFlow' (a newline).\n\nThe actual token that comes after 'Machine learning with TensorFlow can' is 'do', and not the predicted 'be'. Compute the _conditional_ probability of the token 'do', and compare it to that of the token 'be'. \n    \n","metadata":{}},{"cell_type":"code","source":"# compute the conditional probabilities of the tokens given 'Machine learning with TensorFlow can'\n# YOUR CODE (approx. 1 line of code)\nprobs = tf.keras.activations.softmax(output.logits[0,6:7,:])[0,:].numpy()\n\ntoken_id_of_do = tokenizer.encode(' do')\ntoken_id_of_be = tokenizer.encode(' be')\nratio = probs[token_id_of_be] / probs[token_id_of_do] \n\nprint(f\"The token ' be' is {ratio} times more likely than the token ' do' to follow \\\"Machine learning with TensorFlow can\\\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:00:35.484052Z","iopub.execute_input":"2024-11-22T22:00:35.484442Z","iopub.status.idle":"2024-11-22T22:00:35.493067Z","shell.execute_reply.started":"2024-11-22T22:00:35.484411Z","shell.execute_reply":"2024-11-22T22:00:35.491978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check your work (Run this to get points!)\npart_2.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#part_2.hint()\n#part_2.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:00:36.777166Z","iopub.execute_input":"2024-11-22T22:00:36.777907Z","iopub.status.idle":"2024-11-22T22:00:36.787353Z","shell.execute_reply.started":"2024-11-22T22:00:36.777876Z","shell.execute_reply":"2024-11-22T22:00:36.786321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 3: Generating text: iteratively predicting the _next word_\n\nLet's use these principles to make GPT2 generate some text by starting from the prompt \"_In the future_\", predict the next token as we did above, concatenating that token to the prompt, and use that as a new prompt. This is set up in the code below, but some code is missing. Complete the code and see what text is generated:\n    \n","metadata":{}},{"cell_type":"code","source":"start_text = tokenizer.encode(\"In the future\")\n\nnum_tokens_generated = 0\nwhile tokenizer.decode(start_text[-1]) != '.': # stop as soon the last predicted token is a period\n    \n    # store list of tokens as a numpy array \n    start_text_as_numpy = np.array(start_text, dtype=\"int32\")\n    \n    ## predict the next token: \n    # compute logits, \n    logits = gpt2_model(start_text_as_numpy).logits\n    \n    # find the one with the largest logit\n    # YOU CODE (~ 1 line of code)\n    most_likely_token = tf.argmax(logits,axis=1)[-1].numpy()\n    \n    # append the generated token to end of the sequence\n    start_text += [most_likely_token]\n    print(tokenizer.decode(start_text))\n    \n    num_tokens_generated += 1\n    if num_tokens_generated > 30:\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check your work (Run this to get points!)\npart_3.check()\n\n# You can ask for a hint or the solution by uncommenting the following:\n#part_3.hint()\n#part_3.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:02:49.382926Z","iopub.execute_input":"2024-11-22T22:02:49.383290Z","iopub.status.idle":"2024-11-22T22:02:49.394079Z","shell.execute_reply.started":"2024-11-22T22:02:49.383259Z","shell.execute_reply":"2024-11-22T22:02:49.393147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### Using `pipeline` to generate text with GPT2\n\nThe [&#129303; HuggingFace documentation](https://huggingface.co/gpt2?text=My+name+is+Thomas+and+my+main) explains that the GPT2 model can also be used for a text generating `pipeline`. \n\nAs you may recall from the [first tutorial](https://www.kaggle.com/datasniffer/nlp-intro) in this course, `pipeline`'s take a string of text, and turns it into whatever it is supposed to turn it into (e.g., a _sentiment classification_, an _answer to a question_, _more text_). A `pipeline` does all the steps required for transforming an input string of text into an output that we have been doing by hand in the past exercises:\n\n1. tokinzing the string, \n2. encoding the tokens, \n3. passing the encoded sequence of tokens through a neural language model\n4. transforming the output to a category label\n\n\nThe \"text-generation\" `pipeline` essentially does what you have implemented in the above code cell. Let's see it in action:\n    \n","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import pipeline\ntransformers.set_seed(42) # for repeatablity\n\ngpt2_generator = pipeline('text-generation', model='gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:05:14.989413Z","iopub.execute_input":"2024-11-22T22:05:14.989669Z","iopub.status.idle":"2024-11-22T22:05:18.632651Z","shell.execute_reply.started":"2024-11-22T22:05:14.989646Z","shell.execute_reply":"2024-11-22T22:05:18.631591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpt2_generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5, pad_token_id=50256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:05:13.155063Z","iopub.execute_input":"2024-11-22T22:05:13.155368Z","iopub.status.idle":"2024-11-22T22:05:14.988390Z","shell.execute_reply.started":"2024-11-22T22:05:13.155342Z","shell.execute_reply":"2024-11-22T22:05:14.987294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nAs you can see, we passed some extra arguments to the text generator. In particular, we passed the argument `num_return_sequences = 5`. The result is that we get 5 generated pieces of textâ€”all starting with our prompt \"Hello, I'm a language model,\", but then all ending differently. You may have noted that when you run your own text generating code, it will always return the same answer to the same prompt. So how does `gpt2_generator` give different results?\n\nThe obvious answer is of course that it introduces randomness. The question then is just \"how?\"\n\n","metadata":{}},{"cell_type":"markdown","source":"\n# Step 4: Introducing random variation\n\nRemember that in our own generating code we used logits to predict the most likely next token. We were able to turn these logits into probabilities by means of the `keras.activations.softmax()` function. Let's consider this in more detail. \n    \nSay we have a the sequence `[\"Hello\", \",\", \"I\", \"'m\", \"a\", \"language\", \"model\"]`. If we enter this sequence into the model we receive as output an object that holds (among other things) the matrix with logits. The _last row_ of this matrix contains the logits $L_i$ for the token in the vocabulary with token ID $i$, _conditional_ on the input sequence, for all $i=0, \\ldots, N_\\text{vocab}-1$. Then\n\n$$\n\\displaystyle P(\\text{next token ID} = i\\,|\\,\\mathtt{Hello, ,, I, }\\text{'}\\mathtt{m, a, language, model}) = {e^{L_i} \\over \\sum_{j=0}^{N_\\text{vocab}-1} e^{L_j}}\n$$\n\nis the _conditional_ probability that the next token ID is $i$, _given_ the prompt \"_Hello, I'm a language model_\" as input. \n\nThe sequence of probabilities $P(T\\,|\\,\\mathtt{Hello, ,, I, 'm, a, language, model})$ for all tokens $T$ in the vocabulary forms a discrete probability distribution. To introduce randomness into the generated text without getting unlikely results, we can simply sample from this distribution. Next, we explore this idea further.\n    \n","metadata":{}},{"cell_type":"markdown","source":"\n#### Random sampling in Tensorflow\n\nTensorflow comes with a function `tf.random.categorical` that can be used to generate random draws from a discrete distribution as just discussed. It accepts as its first argument _not_ probabilities, but logits. These are then used internally to sample in accordance with the probabilities that we see when applying the softmax function to the logits. A second argument specifies the sample size: \n","metadata":{}},{"cell_type":"code","source":"# obtain conditional logits for the next word\ntext = tokenizer.encode(\"Hello, I'm a language\")\ntext_as_numpy = np.array(text, dtype=\"int32\")\nlogits = gpt2_model(text_as_numpy).logits\n\n# generate a random token from the conditional probability distribution\nnext_token = tf.random.categorical(logits, num_samples=1)[-1][0].numpy()\nnext_token, tokenizer.decode(next_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:15:33.588185Z","iopub.execute_input":"2024-11-22T22:15:33.588533Z","iopub.status.idle":"2024-11-22T22:15:33.693558Z","shell.execute_reply.started":"2024-11-22T22:15:33.588504Z","shell.execute_reply":"2024-11-22T22:15:33.692639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nUsing this random sampling function, we now introduce some variation into the sentences that are generated by our `while` loop. Complete the code below to do so:\n    \n","metadata":{}},{"cell_type":"code","source":"random_text = tokenizer.encode(\"In the future\") #\"This is a sports\")\n\nnum_tokens_generated = 0\nwhile tokenizer.decode(random_text[-1]) != '.': # stop as soon the last predicted token is a period\n    \n    # store list of tokens as a numpy array \n    random_text_as_numpy = np.array(random_text, dtype=\"int32\")\n    \n    # predict the next token: compute logits, then take the one with the largest logit\n    logits = gpt2_model(random_text_as_numpy).logits\n    # YOUR CODE (1 line of code)\n    random_next_token = tf.random.categorical(logits, num_samples=1)[-1][0].numpy()\n    \n    # paste generated token to end of the sequence\n    random_text += [random_next_token]\n    print(tokenizer.decode(random_text))\n    \n    num_tokens_generated += 1\n    if num_tokens_generated > 30:\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:17:27.885510Z","iopub.execute_input":"2024-11-22T22:17:27.885885Z","iopub.status.idle":"2024-11-22T22:17:30.453346Z","shell.execute_reply.started":"2024-11-22T22:17:27.885853Z","shell.execute_reply":"2024-11-22T22:17:30.452363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It should be said that the way GPT2 introduces randomness in the generated text is a little bit different that the simple method here.","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\npart_4.solution()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T22:17:37.098705Z","iopub.execute_input":"2024-11-22T22:17:37.099088Z","iopub.status.idle":"2024-11-22T22:17:37.107375Z","shell.execute_reply.started":"2024-11-22T22:17:37.099055Z","shell.execute_reply":"2024-11-22T22:17:37.106364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Congratulations!\n\nYou've finished the NLP course. It's an exciting field that has developed some of the most advanced deep learning models and that will help you make use of vast amounts of data you didn't know how to work with before.\n\nThis course should be just your introduction. Try a project with [text](https://www.kaggle.com/datasets?tags=14104-text+data). You'll have fun with it, and your skills will continue growing.\n    \n","metadata":{}}]}